\documentclass[main.tex]{subfiles}

% Header from main.tex

\begin{document}
\section{Literature Review}
Bike sharing is an up-and-coming transportation method that can be implemented in cities and other regions to offer users the ability to transport themselves quickly where they want to go without the costs and concerns of personal bike ownership as well as helping to reduce the impact of the city on the environment. A problem arises with these systems when the operators need to determine how to best balance the availability of bicycles against monetary losses from bicycle systems with excessive availability.
The dataset\cite{Irvine} has already been examined in two papers\cite{datamining}\cite{rulebased} that we examined so that we would not merely reproduce their models, but be able to learn from their methodology and add to their conclusions by looking at other types of models.\\
In "A rule-based model for Seoul Bike sharing demand prediction using weather data" \cite{rulebased}, the authors collected the data from several APIs and merged them together to get a more complete view of biking activity. After this paper, the authors shared the dataset that they collected with us through the UC Irvine machine learning datasets portal\cite{Irvine}. The authors tried five approaches: CUBIST, regularized random forests, classification and regression trees and K Nearest Neigbors to understand the dataset that they gathered. Of the five methods that they used to attempt to model the dataset, CUBIST showed the most promising results with an $R^2$ of 0.95 on the testing dataset, as well as lower root mean square errors, mean absolute error, and coefficent of variation than the other four methods presented. 
In "Using data mining techniques for bike sharing demand prediction in metropolitan city" \cite{datamining} the authors used the same dataset with all of its added features to determine what was the smallest subset of the data that they could use and with which models could it be used. In the article, the authors explored linear regression, gradient boosting machines, support vector machines, boosted trees, and extreme gradient boosted trees. They found with all of the variables they were able to achieve an $R^2$ of 0.92 on the testing dataset using gradient boosted machines and not using data about several of the days of the week as well as snow. The paper found that weather and categorical data helps greatly to ensure model accuracy finding that removing those can reduce the testing $R^2$ by 0.27 or 0.15. 
\end{document}
